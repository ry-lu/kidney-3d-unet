{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-07T03:07:14.575314Z","iopub.status.busy":"2024-04-07T03:07:14.574820Z","iopub.status.idle":"2024-04-07T03:07:49.463829Z","shell.execute_reply":"2024-04-07T03:07:49.462649Z","shell.execute_reply.started":"2024-04-07T03:07:14.575281Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting volumentations-3D\n","  Obtaining dependency information for volumentations-3D from https://files.pythonhosted.org/packages/59/a6/af48582a42ad57eb7bb6517802c2fb07eaa3a715c6db9d9c356870795bca/volumentations_3D-1.0.4-py3-none-any.whl.metadata\n","  Downloading volumentations_3D-1.0.4-py3-none-any.whl.metadata (481 bytes)\n","Requirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from volumentations-3D) (0.21.0)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from volumentations-3D) (1.11.4)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from volumentations-3D) (4.8.1.78)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from volumentations-3D) (1.24.3)\n","Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->volumentations-3D) (3.1)\n","Requirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->volumentations-3D) (10.1.0)\n","Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->volumentations-3D) (2.31.1)\n","Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->volumentations-3D) (2023.8.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->volumentations-3D) (1.4.1)\n","Requirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->volumentations-3D) (21.3)\n","Requirement already satisfied: lazy_loader>=0.2 in /opt/conda/lib/python3.10/site-packages (from scikit-image->volumentations-3D) (0.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->volumentations-3D) (3.0.9)\n","Downloading volumentations_3D-1.0.4-py3-none-any.whl (31 kB)\n","Installing collected packages: volumentations-3D\n","Successfully installed volumentations-3D-1.0.4\n","Collecting segmentation_models_pytorch\n","  Obtaining dependency information for segmentation_models_pytorch from https://files.pythonhosted.org/packages/cb/70/4aac1b240b399b108ce58029ae54bc14497e1bbc275dfab8fd3c84c1e35d/segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata\n","  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata (30 kB)\n","Requirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.15.1+cpu)\n","Collecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting efficientnet-pytorch==0.7.1 (from segmentation_models_pytorch)\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting timm==0.9.2 (from segmentation_models_pytorch)\n","  Obtaining dependency information for timm==0.9.2 from https://files.pythonhosted.org/packages/29/90/94f5deb8d76e24a89813aef95e8809ca8fd7414490428480eda19b133d4a/timm-0.9.2-py3-none-any.whl.metadata\n","  Downloading timm-0.9.2-py3-none-any.whl.metadata (68 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (4.66.1)\n","Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (10.1.0)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.0.0+cpu)\n","Collecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\n","  Obtaining dependency information for munch from https://files.pythonhosted.org/packages/56/b3/7c69b37f03260a061883bec0e7b05be7117c1b1c85f5212c72c8c2bc3c8c/munch-4.0.0-py2.py3-none-any.whl.metadata\n","  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.19.4)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.1)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.24.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.5.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (2023.12.1)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (21.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2023.11.17)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (3.0.9)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n","Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n","Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=f3e1d3a74cbda3d1f4cdcab215557cbd87d5ba89c775907bc64045593aabd1bc\n","  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60943 sha256=3ad3e5ef65aa2cf468579975384bb2a9ad421e2952b4f7bd65b8e5f9d2353f0a\n","  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n","Successfully built efficientnet-pytorch pretrainedmodels\n","Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\n","  Attempting uninstall: timm\n","    Found existing installation: timm 0.9.12\n","    Uninstalling timm-0.9.12:\n","      Successfully uninstalled timm-0.9.12\n","Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.3 timm-0.9.2\n"]}],"source":["!pip install volumentations-3D\n","!pip install segmentation_models_pytorch"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T03:08:34.944746Z","iopub.status.busy":"2024-04-07T03:08:34.944365Z","iopub.status.idle":"2024-04-07T03:08:34.949960Z","shell.execute_reply":"2024-04-07T03:08:34.948823Z","shell.execute_reply.started":"2024-04-07T03:08:34.944709Z"},"trusted":true},"outputs":[],"source":["from __future__ import annotations\n","# ====================================================\n","# Directory settings\n","# ====================================================\n","import os\n","from pathlib import Path\n","\n","OUTPUT_DIR = '/kaggle/working'\n","\n","INPUT_DIR='/kaggle/input/blood-vessel-segmentation'"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T03:08:36.446040Z","iopub.status.busy":"2024-04-07T03:08:36.445674Z","iopub.status.idle":"2024-04-07T03:08:36.453318Z","shell.execute_reply":"2024-04-07T03:08:36.452216Z","shell.execute_reply.started":"2024-04-07T03:08:36.446014Z"},"trusted":true},"outputs":[],"source":["# ====================================================\n","# CFG\n","# ====================================================\n","class CFG:\n","    apex=True\n","    wandb = False\n","    competition = \"HOA3D\"\n","    backbone = 'resnet18d'\n","    max_grad_norm = 1000\n","    debug = False\n","    debug_train_size = 200\n","    scheduler = \"cosine\"\n","    num_warmup_steps=0\n","    epochs = 2\n","    decoder_lr = 0.005\n","    betas = (0.9, 0.999)\n","    batch_size = 4\n","    infer_batch_size = 4\n","    ckpt_name = 'unet3d-baseline'\n","    weight_decay = 0.1\n","    seed = 42\n","    print_freq=50\n","    eval_freq =1000\n","    eval_step_save_start_epoch=0\n","    train = True\n","\n","if CFG.debug:\n","    CFG.epochs = 1"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T03:08:38.245889Z","iopub.status.busy":"2024-04-07T03:08:38.245194Z","iopub.status.idle":"2024-04-07T03:08:46.356316Z","shell.execute_reply":"2024-04-07T03:08:46.355451Z","shell.execute_reply.started":"2024-04-07T03:08:38.245855Z"},"trusted":true},"outputs":[],"source":["# ====================================================\n","# Library\n","# ====================================================\n","import os\n","import gc\n","import re\n","import ast\n","import sys\n","import copy\n","import json\n","import time\n","import math\n","import string\n","import pickle\n","import random\n","import itertools\n","import glob\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","from tqdm.auto import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD, AdamW\n","from torch.utils.data import DataLoader, Dataset\n","import timm\n","import segmentation_models_pytorch as smp\n","\n","from PIL import Image\n","import cv2\n","\n","from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T03:08:46.358412Z","iopub.status.busy":"2024-04-07T03:08:46.357861Z","iopub.status.idle":"2024-04-07T03:08:46.366981Z","shell.execute_reply":"2024-04-07T03:08:46.366268Z","shell.execute_reply.started":"2024-04-07T03:08:46.358387Z"},"trusted":true},"outputs":[],"source":["def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","seed_everything(seed=42)"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T03:08:46.368497Z","iopub.status.busy":"2024-04-07T03:08:46.367912Z","iopub.status.idle":"2024-04-07T03:08:46.385874Z","shell.execute_reply":"2024-04-07T03:08:46.385038Z","shell.execute_reply.started":"2024-04-07T03:08:46.368462Z"},"trusted":true},"outputs":[],"source":["def extract_3d_voxels_for_patches(image_3d, patch_size=(128, 128, 128), stride=(64, 64, 64)):\n","    \"\"\"\n","    Extracts patches in an image and returns a list of lowest voxels.\n","    \"\"\"\n","    patches = []\n","\n","    for start_x in range(0, image_3d.shape[0] - patch_size[0], stride[0]):\n","        for start_y in range(0, image_3d.shape[1] - patch_size[1], stride[1]):\n","            for start_z in range(0, image_3d.shape[2] - patch_size[2], stride[2]):\n","                # Lowest voxel needed to extract patch\n","                lowest_voxel = (start_x,start_y,start_z)\n","                patches.append(lowest_voxel)\n","                \n","    return patches\n","\n","def extract_patch_from_voxel(image_3d, lowest_voxel, patch_size=(128, 128, 128)):\n","    \"\"\"\n","    Extracts a 3d patch given the lowest voxel.\n","    \"\"\"\n","    end_x = lowest_voxel[0] + patch_size[0] \n","    end_y = lowest_voxel[1] + patch_size[1]\n","    end_z = lowest_voxel[2] + patch_size[2]\n","\n","    patch = image_3d[lowest_voxel[0]:end_x,\n","                     lowest_voxel[1]:end_y,\n","                     lowest_voxel[2]:end_z]\n","\n","    return patch\n","\n","\n","\n","def filter_empty_patches_by_voxel(lowest_voxels, mask, patch_size, threshold = 0):\n","    \"\"\"\n","    Removes the lowest voxels that represent patches with positive values <= threshold\n","    \"\"\"\n","    positive_voxels = []\n","\n","    for lowest_voxel in tqdm(lowest_voxels):\n","        # Extract the patch\n","        patch = mask[lowest_voxel[0]:lowest_voxel[0]  + patch_size[0],\n","                         lowest_voxel[1]:lowest_voxel[1]  + patch_size[1],\n","                         lowest_voxel[2]:lowest_voxel[2]  + patch_size[2]]\n","\n","        if np.count_nonzero(patch) > threshold:\n","            positive_voxels.append(lowest_voxel)\n","\n","\n","    return positive_voxels"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T22:21:36.372574Z","iopub.status.busy":"2023-12-18T22:21:36.372300Z","iopub.status.idle":"2023-12-18T22:21:36.389814Z","shell.execute_reply":"2023-12-18T22:21:36.388922Z","shell.execute_reply.started":"2023-12-18T22:21:36.372552Z"},"trusted":true},"outputs":[],"source":["class ValidKidney3DDataset(Dataset):\n","    def __init__(self, patches, masks,patch_size,stride_size, transformations=None):\n","        self.patches = patches\n","        self.masks = masks\n","        self.patch_size = patch_size\n","        self.lowest_voxels = extract_3d_voxels_for_patches(patches, patch_size=patch_size, stride=stride_size)\n","        self.lowest_voxels = filter_empty_patches_by_voxel(self.lowest_voxels, masks, patch_size,threshold=0)\n","        self.transformations = transformations\n","        \n","    def __len__(self):\n","        return len(self.lowest_voxels)\n","\n","    def __getitem__(self, idx):\n","        image = extract_patch_from_voxel(self.patches,self.lowest_voxels[idx],patch_size=self.patch_size)\n","        mask = extract_patch_from_voxel(self.masks,self.lowest_voxels[idx],patch_size=self.patch_size)\n","        \n","        image = np.expand_dims(image,0)\n","        mask = np.expand_dims(mask,0)\n","        \n","        data = {'image':image,'mask': mask}\n","        if self.transformations:\n","            data = self.transformations(**data)\n","            \n","        data['image']=(data['image'] - data['image'].mean()) / (data['image'].std() + 0.0001)\n","\n","        data['image'] = torch.tensor(data['image'], dtype=torch.float)\n","        data['mask'] = torch.tensor(data['mask'], dtype=torch.float)\n","        return data"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T22:21:36.391224Z","iopub.status.busy":"2023-12-18T22:21:36.390942Z","iopub.status.idle":"2023-12-18T22:21:36.404608Z","shell.execute_reply":"2023-12-18T22:21:36.403883Z","shell.execute_reply.started":"2023-12-18T22:21:36.391200Z"},"trusted":true},"outputs":[],"source":["class TrainKidney3DDataset(Dataset):\n","    def __init__(self, patches, masks, lowest_voxels, patch_size, transformations=None):\n","        self.patches = patches\n","        self.masks = masks\n","        self.lowest_voxels = lowest_voxels\n","        self.patch_size = patch_size\n","        self.transformations = transformations\n","        \n","    def __len__(self):\n","        return len(self.lowest_voxels)\n","\n","    def __getitem__(self, idx):\n","        image = extract_patch_from_voxel(self.patches,self.lowest_voxels[idx],patch_size=self.patch_size)\n","        mask = extract_patch_from_voxel(self.masks,self.lowest_voxels[idx], patch_size=self.patch_size)\n","        \n","        image = np.expand_dims(image,0)\n","        mask = np.expand_dims(mask,0)\n","    \n","        data = {'image':image,'mask': mask}\n","        if self.transformations:\n","            data = self.transformations(**data)\n","        \n","        \n","        data['image']=(data['image'] - data['image'].mean()) / (data['image'].std() + 0.0001)\n","        data['image'] = torch.tensor(data['image'], dtype=torch.float)\n","        data['mask'] = torch.tensor(data['mask'], dtype=torch.float)\n","        return data"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T03:09:29.657777Z","iopub.status.busy":"2024-04-07T03:09:29.657419Z","iopub.status.idle":"2024-04-07T03:09:30.328963Z","shell.execute_reply":"2024-04-07T03:09:30.327785Z","shell.execute_reply.started":"2024-04-07T03:09:29.657751Z"},"trusted":true},"outputs":[],"source":["import volumentations as volumen\n","\n","def get_augmentation(patch_size):\n","    return volumen.Compose([\n","        volumen.RandomGamma(gamma_limit=(80, 120), p=0.3),\n","        volumen.GaussianNoise(var_limit=(0, 5), p=0.3),\n","        volumen.Flip(1, p=0.3),\n","        volumen.Flip(2, p=0.3),\n","    ], p=1.0)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T03:09:33.487058Z","iopub.status.busy":"2024-04-07T03:09:33.486573Z","iopub.status.idle":"2024-04-07T03:09:33.491206Z","shell.execute_reply":"2024-04-07T03:09:33.490288Z","shell.execute_reply.started":"2024-04-07T03:09:33.487032Z"},"trusted":true},"outputs":[],"source":["patch_size = (128,128,128)\n","stride = (32,32,32)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T03:09:46.571015Z","iopub.status.busy":"2024-04-07T03:09:46.569903Z"},"trusted":true},"outputs":[],"source":["\n","# npz version of data\n","kidney3 = np.load('/kaggle/input/sennet-hoa-all-kidneys-dense/kidney_3_dense.npz')\n","kidney3_volume = kidney3['volume'].astype(np.uint8)\n","kidney3_masks = kidney3['mask'].astype(np.uint8)\n","\n","kidney1 = np.load('/kaggle/input/sennet-hoa-all-kidneys-dense/kidney_1_dense.npz')\n","kidney1_volume = kidney1['volume'].astype(np.uint8)\n","kidney1_masks = kidney1['mask'].astype(np.uint8)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["my_voxels = extract_3d_voxels_for_patches(kidney1_volume,patch_size=patch_size,stride=(32,32,32))\n","my_voxels = filter_empty_patches_by_voxel(my_voxels,kidney1_masks,patch_size=patch_size,threshold=50)\n","np.save('/kaggle/working/kidney1_lowest_voxel_patches_stride32_thresh50.npy', my_voxels)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T22:23:34.626607Z","iopub.status.busy":"2023-12-18T22:23:34.626348Z","iopub.status.idle":"2023-12-18T22:23:34.674166Z","shell.execute_reply":"2023-12-18T22:23:34.673308Z","shell.execute_reply.started":"2023-12-18T22:23:34.626584Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(36872, 3)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# patch size 128x128x128 stride 32x32x32, pos filter thres 50\n","loaded_lowest_voxels = np.load('/kaggle/working/kidney1_lowest_voxel_patches_stride32_thresh50.npy') \n","loaded_lowest_voxels.shape"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T22:23:34.675607Z","iopub.status.busy":"2023-12-18T22:23:34.675333Z","iopub.status.idle":"2023-12-18T22:23:34.680365Z","shell.execute_reply":"2023-12-18T22:23:34.679507Z","shell.execute_reply.started":"2023-12-18T22:23:34.675583Z"},"trusted":true},"outputs":[],"source":["train_dataset = TrainKidney3DDataset(\n","    patches=kidney1_volume,\n","    masks=kidney1_masks,\n","    lowest_voxels=loaded_lowest_voxels,\n","    patch_size=patch_size,\n","    transformations=get_augmentation(patch_size),\n",")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T22:23:34.681870Z","iopub.status.busy":"2023-12-18T22:23:34.681446Z","iopub.status.idle":"2023-12-18T22:23:34.694833Z","shell.execute_reply":"2023-12-18T22:23:34.694042Z","shell.execute_reply.started":"2023-12-18T22:23:34.681838Z"},"trusted":true},"outputs":[{"data":{"text/plain":["36872"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["len(train_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"markdown","metadata":{},"source":["Adapted From First Stage RSNA 2022 Cervical Spine Fracture Detection Qishen Hai"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T22:23:34.696152Z","iopub.status.busy":"2023-12-18T22:23:34.695855Z","iopub.status.idle":"2023-12-18T22:23:34.706349Z","shell.execute_reply":"2023-12-18T22:23:34.705486Z","shell.execute_reply.started":"2023-12-18T22:23:34.696128Z"},"trusted":true},"outputs":[],"source":["n_blocks = 4\n","out_dim = 1\n","class TimmSegModel(nn.Module):\n","    def __init__(self, backbone, segtype='unet', pretrained=False):\n","        super(TimmSegModel, self).__init__()\n","\n","        self.encoder = timm.create_model(\n","            backbone,\n","            in_chans=1,\n","            features_only=True,\n","            drop_rate=0,\n","            drop_path_rate=0,\n","            pretrained=pretrained\n","        )\n","        \n","        \n","        g = self.encoder(torch.rand(1, 1, 64, 64))\n","        encoder_channels = [1] + [_.shape[1] for _ in g]\n","        print(encoder_channels)\n","        decoder_channels = [256, 128, 64, 32, 16]\n","        if segtype == 'unet':\n","            self.decoder = smp.decoders.unet.decoder.UnetDecoder(\n","                encoder_channels=encoder_channels[:n_blocks+1],\n","                decoder_channels=decoder_channels[:n_blocks],\n","                n_blocks=n_blocks,\n","            )\n","\n","        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","\n","    def forward(self,x):\n","        global_features = [0] + self.encoder(x)[:n_blocks]\n","        seg_features = self.decoder(*global_features)\n","        seg_features = self.segmentation_head(seg_features)\n","        return seg_features"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T22:23:34.707773Z","iopub.status.busy":"2023-12-18T22:23:34.707511Z","iopub.status.idle":"2023-12-18T22:23:34.727887Z","shell.execute_reply":"2023-12-18T22:23:34.726943Z","shell.execute_reply.started":"2023-12-18T22:23:34.707750Z"},"trusted":true},"outputs":[],"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from typing import Tuple, Optional, List\n","\n","\n","# Calculate symmetric padding for a convolution\n","def get_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> int:\n","    padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n","    return padding\n","\n","\n","# Calculate asymmetric TensorFlow-like 'SAME' padding for a convolution\n","def get_same_padding(x: int, k: int, s: int, d: int):\n","    return max((math.ceil(x / s) - 1) * s + (k - 1) * d + 1 - x, 0)\n","\n","\n","# Can SAME padding for given args be done statically?\n","def is_static_pad(kernel_size: int, stride: int = 1, dilation: int = 1, **_):\n","    return stride == 1 and (dilation * (kernel_size - 1)) % 2 == 0\n","\n","\n","# Dynamically pad input x with 'SAME' padding for conv with specified args\n","def pad_same(x, k: List[int], s: List[int], d: List[int] = (1, 1, 1), value: float = 0):\n","    ih, iw, iz = x.size()[-3:]\n","    pad_h = get_same_padding(ih, k[0], s[0], d[0])\n","    pad_w = get_same_padding(iw, k[1], s[1], d[1])\n","    pad_z = get_same_padding(iz, k[2], s[2], d[2])\n","    if pad_h > 0 or pad_w > 0 or pad_z > 0:\n","        x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2, pad_z // 2, pad_z - pad_z // 2], value=value)\n","    return x\n","\n","\n","def get_padding_value(padding, kernel_size, **kwargs) -> Tuple[Tuple, bool]:\n","    dynamic = False\n","    if isinstance(padding, str):\n","        # for any string padding, the padding will be calculated for you, one of three ways\n","        padding = padding.lower()\n","        if padding == 'same':\n","            # TF compatible 'SAME' padding, has a performance and GPU memory allocation impact\n","            if is_static_pad(kernel_size, **kwargs):\n","                # static case, no extra overhead\n","                padding = get_padding(kernel_size, **kwargs)\n","            else:\n","                # dynamic 'SAME' padding, has runtime/GPU memory overhead\n","                padding = 0\n","                dynamic = True\n","        elif padding == 'valid':\n","            # 'VALID' padding, same as padding=0\n","            padding = 0\n","        else:\n","            # Default to PyTorch style 'same'-ish symmetric padding\n","            padding = get_padding(kernel_size, **kwargs)\n","    return padding, dynamic\n","\n","\n","def conv3d_same(\n","        x, weight: torch.Tensor, bias: Optional[torch.Tensor] = None, stride: Tuple[int, int, int] = (1, 1, 1),\n","        padding: Tuple[int, int, int] = (0, 0, 0), dilation: Tuple[int, int, int] = (1, 1, 1), groups: int = 1):\n","    x = pad_same(x, weight.shape[-3:], stride, dilation)\n","    return F.conv3d(x, weight, bias, stride, (0, 0, 0), dilation, groups)\n","\n","\n","class Conv3dSame(nn.Conv3d):\n","    \"\"\" Tensorflow like 'SAME' convolution wrapper for 3d convolutions\n","    \"\"\"\n","\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n","                 padding=0, dilation=1, groups=1, bias=True):\n","        super(Conv3dSame, self).__init__(\n","            in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n","\n","    def forward(self, x):\n","        return conv3d_same(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n","\n","\n","def create_conv3d_pad(in_chs, out_chs, kernel_size, **kwargs):\n","    padding = kwargs.pop('padding', '')\n","    kwargs.setdefault('bias', False)\n","    padding, is_dynamic = get_padding_value(padding, kernel_size, **kwargs)\n","    if is_dynamic:\n","        return Conv3dSame(in_chs, out_chs, kernel_size, **kwargs)\n","    else:\n","        return nn.Conv3d(in_chs, out_chs, kernel_size, padding=padding, **kwargs)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T22:23:34.810714Z","iopub.status.busy":"2023-12-18T22:23:34.810481Z","iopub.status.idle":"2023-12-18T22:23:34.825602Z","shell.execute_reply":"2023-12-18T22:23:34.824830Z","shell.execute_reply.started":"2023-12-18T22:23:34.810693Z"},"trusted":true},"outputs":[],"source":["from timm.layers.conv2d_same import Conv2dSame\n","\n","def convert_3d(module):\n","\n","    module_output = module\n","    if isinstance(module, torch.nn.BatchNorm2d):\n","        module_output = torch.nn.BatchNorm3d(\n","            module.num_features,\n","            module.eps,\n","            module.momentum,\n","            module.affine,\n","            module.track_running_stats,\n","        )\n","        if module.affine:\n","            with torch.no_grad():\n","                module_output.weight = module.weight\n","                module_output.bias = module.bias\n","        module_output.running_mean = module.running_mean\n","        module_output.running_var = module.running_var\n","        module_output.num_batches_tracked = module.num_batches_tracked\n","        if hasattr(module, \"qconfig\"):\n","            module_output.qconfig = module.qconfig\n","            \n","    elif isinstance(module, Conv2dSame):\n","        module_output = Conv3dSame(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.Conv2d):\n","        module_output = torch.nn.Conv3d(\n","            in_channels=module.in_channels,\n","            out_channels=module.out_channels,\n","            kernel_size=module.kernel_size[0],\n","            stride=module.stride[0],\n","            padding=module.padding[0],\n","            dilation=module.dilation[0],\n","            groups=module.groups,\n","            bias=module.bias is not None,\n","            padding_mode=module.padding_mode\n","        )\n","        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n","\n","    elif isinstance(module, torch.nn.MaxPool2d):\n","        module_output = torch.nn.MaxPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            dilation=module.dilation,\n","            ceil_mode=module.ceil_mode,\n","        )\n","    elif isinstance(module, torch.nn.AvgPool2d):\n","        module_output = torch.nn.AvgPool3d(\n","            kernel_size=module.kernel_size,\n","            stride=module.stride,\n","            padding=module.padding,\n","            ceil_mode=module.ceil_mode,\n","        )\n","\n","    for name, child in module.named_children():\n","        module_output.add_module(\n","            name, convert_3d(child)\n","        )\n","    del module\n","\n","    return module_output"]},{"cell_type":"markdown","metadata":{},"source":["# Loss"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T22:23:34.827081Z","iopub.status.busy":"2023-12-18T22:23:34.826795Z","iopub.status.idle":"2023-12-18T22:23:34.843040Z","shell.execute_reply":"2023-12-18T22:23:34.842165Z","shell.execute_reply.started":"2023-12-18T22:23:34.827056Z"},"trusted":true},"outputs":[],"source":["class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        \n","        inputs = F.sigmoid(inputs)       \n","        \n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        \n","        intersection = (inputs * targets).sum()                            \n","        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n","        \n","        return 1 - dice\n","    \n","\n","class BCELoss(nn.Module):\n","    def __init__(self, smooth=1.0, pos_weight=1, device='cpu'):\n","        super(BCELoss, self).__init__()\n","        self.smooth = smooth\n","        self.pos_weight = pos_weight\n","        self.device = device\n","\n","    def forward(self, inputs, targets):\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        pos_weight = torch.tensor([self.pos_weight]).to(self.device)\n","        loss = F.binary_cross_entropy_with_logits(inputs, targets,pos_weight=pos_weight)\n","        return loss\n","    \n","\n","class FocalLoss(nn.modules.loss._WeightedLoss):\n","\n","    def __init__(self, gamma=0, size_average=None, ignore_index=-100,\n","                 reduce=None, balance_param=1.0):\n","        super(FocalLoss, self).__init__(size_average)\n","        self.gamma = gamma\n","        self.size_average = size_average\n","        self.ignore_index = ignore_index\n","        self.balance_param = balance_param\n","\n","    def forward(self, input, target):\n","        logpt = - F.binary_cross_entropy_with_logits(input, target)\n","        pt = torch.exp(logpt)\n","\n","        focal_loss = -((1 - pt) ** self.gamma) * logpt\n","        balanced_focal_loss = self.balance_param * focal_loss\n","        return balanced_focal_loss\n","    \n","class DiceBCELoss(nn.Module):\n","    def __init__(self, weight=None, pos_weight=1, device='cpu'):\n","        super(DiceBCELoss, self).__init__()\n","        self.pos_weight = pos_weight\n","        self.device = device\n","        \n","    def forward(self, inputs, targets, smooth=1):\n","        \n","        inputs = F.sigmoid(inputs)       \n","        \n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        \n","        intersection = (inputs * targets).sum()                            \n","        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n","        \n","        pos_weight = torch.tensor([self.pos_weight]).to(self.device)\n","        bce_loss = F.binary_cross_entropy(inputs, targets, pos_weight=pos_weight,reduction='mean')\n","        Dice_BCE = (bce_loss + dice_loss)/2\n","        \n","        return Dice_BCE"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T22:23:34.844858Z","iopub.status.busy":"2023-12-18T22:23:34.844499Z","iopub.status.idle":"2023-12-18T22:23:34.867100Z","shell.execute_reply":"2023-12-18T22:23:34.866169Z","shell.execute_reply.started":"2023-12-18T22:23:34.844827Z"},"trusted":true},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","    \n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n","\n","def train_fn(\n","    train_loader: DataLoader,\n","    valid_loader: DataLoader,\n","    model: nn.Module,\n","    criterion: nn.Module,\n","    optimizer,\n","    epoch: int,\n","    scheduler,\n","    device,\n","    best_score,\n",") -> torch.Tensor:\n","\n","\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = end = time.time()\n","    global_step = 0\n","    for step, (batch) in enumerate(train_loader):\n","        \n","        \n","        inputs = batch['image'].to(device)\n","        labels = batch['mask'].to(device)\n","\n","        batch_size = labels.size(0)\n","        \n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            y_preds = model(inputs)\n","            loss = criterion(y_preds, labels)\n","            \n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(\n","            model.parameters(), CFG.max_grad_norm\n","        )\n","\n","        scaler.step(optimizer)\n","        scaler.update()\n","        optimizer.zero_grad()\n","        global_step += 1\n","        scheduler.step()\n"," \n","        end = time.time()\n","        \n","        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n","            print(\n","                \"Epoch: [{0}][{1}/{2}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n","                \"Grad: {grad_norm:.4f}  \"\n","                \"LR: {lr:.8f}  \".format(\n","                    epoch + 1,\n","                    step,\n","                    len(train_loader),\n","                    remain=timeSince(start, float(step + 1) / len(train_loader)),\n","                    loss=losses,\n","                    grad_norm=grad_norm,\n","                    lr=scheduler.get_lr()[0],\n","                )\n","            )\n","            \n","            \n","                \n","        if CFG.eval_step_save_start_epoch <= epoch and (\n","            (step + 1) % CFG.eval_freq == 0\n","        ):\n","            val_loss = valid_fn(valid_loader, model, criterion, device)\n","            score = val_loss\n","            if score < best_score:\n","                best_score = score\n","                save_ckpt(model)\n","                print(f\"Saving New Best Score Model\")\n","            \n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return losses.avg, best_score\n","\n","\n","@torch.inference_mode()\n","def valid_fn(\n","    valid_loader: DataLoader, model: nn.Module, criterion: nn.Module, device\n",") -> tuple[torch.Tensor, np.ndarray]:\n","    \n","    losses = AverageMeter()\n","    model.eval()\n","    start = end = time.time()\n","    for step, (batch) in enumerate(valid_loader):\n","        inputs = batch['image'].to(device)\n","        labels = batch['mask'].to(device)\n","        batch_size = labels.size(0)\n","        \n","        y_preds = model(inputs)\n","            \n","        loss = criterion(y_preds, labels)\n","    \n","            \n","        losses.update(loss.item(), batch_size)\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n","            print(\n","                \"EVAL: [{0}/{1}] \"\n","                \"Elapsed {remain:s} \"\n","                \"Loss: {loss.val:.4f}({loss.avg:.4f}) \".format(\n","                    step,\n","                    len(valid_loader),\n","                    loss=losses,\n","                    remain=timeSince(start, float(step + 1) / len(valid_loader)),\n","                )\n","            )\n","\n","\n","    model.train()\n","    return losses.avg\n","\n","def save_ckpt(\n","    model: torch.nn.Module,\n",") -> None:\n","\n","    save_path = OUTPUT_DIR + f'/{CFG.ckpt_name}.pth'\n","\n","    torch.save(\n","        {\"model\": model.state_dict()},\n","        save_path,\n","    )"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T22:23:34.869758Z","iopub.status.busy":"2023-12-18T22:23:34.868225Z","iopub.status.idle":"2023-12-18T22:23:38.860453Z","shell.execute_reply":"2023-12-18T22:23:38.859428Z","shell.execute_reply.started":"2023-12-18T22:23:34.869729Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 64, 64, 128, 256, 512]\n","Trainable parameters: 39722049\n"]}],"source":["model = TimmSegModel(CFG.backbone)\n","model = convert_3d(model)\n","\n","model.to(device)\n","total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"Trainable parameters: {total_params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["aug = get_augmentation(patch_size)\n","train_dataset = TrainKidney3DDataset(\n","    patches=kidney1_volume,\n","    masks=kidney1_masks,\n","    lowest_voxels=loaded_lowest_voxels,\n","    patch_size=patch_size,\n","    transformations=get_augmentation(patch_size),\n",")\n","\n","valid_dataset = ValidKidney3DDataset(\n","    patches=kidney3_volume,\n","    masks=kidney3_masks,\n","    patch_size=patch_size,\n","    stride_size=(64, 64, 64),\n",")\n","\n","\n","train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=CFG.infer_batch_size, shuffle=False)\n","\n","# Loss function and optimizer\n","# criterion = BCELoss(pos_weight=120,device=device)\n","criterion = DiceLoss()\n","optimizer = AdamW(model.parameters(), lr=CFG.decoder_lr)\n","num_train_steps = int(len(train_dataset) / CFG.batch_size * CFG.epochs)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_train_steps//2)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T22:23:40.672133Z","iopub.status.busy":"2023-12-18T22:23:40.671830Z","iopub.status.idle":"2023-12-18T22:23:40.677902Z","shell.execute_reply":"2023-12-18T22:23:40.677002Z","shell.execute_reply.started":"2023-12-18T22:23:40.672106Z"},"trusted":true},"outputs":[{"data":{"text/plain":["18436"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["num_train_steps"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-12-18T22:23:40.683537Z","iopub.status.busy":"2023-12-18T22:23:40.683186Z","iopub.status.idle":"2023-12-18T22:24:34.186464Z","shell.execute_reply":"2023-12-18T22:24:34.185050Z","shell.execute_reply.started":"2023-12-18T22:23:40.683505Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/9218] Elapsed 0m 8s (remain 1256m 57s) Loss: 0.9894(0.9894) Grad: 16459.8984  LR: 0.00500000  \n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(CFG\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m      3\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m     avg_loss, best_score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     avg_val_loss \u001b[38;5;241m=\u001b[39m valid_fn(valid_loader, model, criterion, device)\n\u001b[1;32m     18\u001b[0m     elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n","Cell \u001b[0;32mIn[24], line 66\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, valid_loader, model, criterion, optimizer, epoch, scheduler, device, best_score)\u001b[0m\n\u001b[1;32m     64\u001b[0m losses\u001b[38;5;241m.\u001b[39mupdate(loss\u001b[38;5;241m.\u001b[39mitem(), batch_size)\n\u001b[1;32m     65\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 66\u001b[0m grad_norm \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m#print(loss)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#print(torch.isnan(y_preds).any().item())\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#print(torch.isnan(labels).any().item())\u001b[39;00m\n\u001b[1;32m     73\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:76\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ((device, _), [grads]) \u001b[38;5;129;01min\u001b[39;00m grouped_grads\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m foreach) \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(grads, device\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[0;32m---> 76\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_coef_clamped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach=True was passed, but can\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["best_score = np.inf\n","for epoch in range(CFG.epochs):\n","    start_time = time.time()\n","    avg_loss, best_score = train_fn(\n","    train_loader,\n","    valid_loader,\n","    model,\n","    criterion,\n","    optimizer,\n","    epoch,\n","    scheduler,\n","    device,\n","    best_score,\n","    )\n","            \n","            \n","    avg_val_loss = valid_fn(valid_loader, model, criterion, device)\n","    elapsed = time.time() - start_time\n","    save_ckpt(model)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T18:35:36.160181Z","iopub.status.busy":"2023-12-17T18:35:36.159802Z","iopub.status.idle":"2023-12-17T18:35:36.377410Z","shell.execute_reply":"2023-12-17T18:35:36.376170Z","shell.execute_reply.started":"2023-12-17T18:35:36.160149Z"},"trusted":true},"outputs":[],"source":["gc.collect()\n","torch.cuda.empty_cache()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":6962461,"sourceId":61446,"sourceType":"competition"},{"datasetId":4130802,"sourceId":7183973,"sourceType":"datasetVersion"},{"datasetId":4162909,"sourceId":7212439,"sourceType":"datasetVersion"},{"sourceId":154437466,"sourceType":"kernelVersion"}],"dockerImageVersionId":30616,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
